{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6pgFbtoN0Nvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c0bd3f-e95f-4443-f7e5-e92c18d5593b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# Load and preprocess data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "13fWYcQL0OfS"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "datagen.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UMeXGqOP0XlE"
      },
      "outputs": [],
      "source": [
        "# Model Architecture\n",
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=(28, 28, 1)))\n",
        "\n",
        "    # Conv Block 1\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Conv Block 2\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Dense Block\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgLk2NGD0cRE",
        "outputId": "b85fa9d0-15d0-4c20-fea9-8a9c23d0056b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "844/844 - 28s - 33ms/step - accuracy: 0.9599 - loss: 0.4422 - val_accuracy: 0.9775 - val_loss: 0.2267 - learning_rate: 1.0000e-03\n",
            "Epoch 2/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9806 - loss: 0.1848 - val_accuracy: 0.9765 - val_loss: 0.1886 - learning_rate: 1.0000e-03\n",
            "Epoch 3/20\n",
            "844/844 - 10s - 12ms/step - accuracy: 0.9832 - loss: 0.1731 - val_accuracy: 0.9812 - val_loss: 0.1802 - learning_rate: 1.0000e-03\n",
            "Epoch 4/20\n",
            "844/844 - 11s - 13ms/step - accuracy: 0.9831 - loss: 0.1821 - val_accuracy: 0.9907 - val_loss: 0.1571 - learning_rate: 1.0000e-03\n",
            "Epoch 5/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9857 - loss: 0.1624 - val_accuracy: 0.9865 - val_loss: 0.1515 - learning_rate: 1.0000e-03\n",
            "Epoch 6/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9861 - loss: 0.1622 - val_accuracy: 0.9887 - val_loss: 0.1569 - learning_rate: 1.0000e-03\n",
            "Epoch 7/20\n",
            "844/844 - 10s - 12ms/step - accuracy: 0.9872 - loss: 0.1541 - val_accuracy: 0.9877 - val_loss: 0.1646 - learning_rate: 1.0000e-03\n",
            "Epoch 8/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9885 - loss: 0.1497 - val_accuracy: 0.9862 - val_loss: 0.1455 - learning_rate: 1.0000e-03\n",
            "Epoch 9/20\n",
            "844/844 - 10s - 12ms/step - accuracy: 0.9879 - loss: 0.1475 - val_accuracy: 0.9888 - val_loss: 0.1425 - learning_rate: 1.0000e-03\n",
            "Epoch 10/20\n",
            "844/844 - 10s - 12ms/step - accuracy: 0.9894 - loss: 0.1384 - val_accuracy: 0.9882 - val_loss: 0.1510 - learning_rate: 1.0000e-03\n",
            "Epoch 11/20\n",
            "844/844 - 10s - 11ms/step - accuracy: 0.9898 - loss: 0.1347 - val_accuracy: 0.9898 - val_loss: 0.1507 - learning_rate: 1.0000e-03\n",
            "Epoch 12/20\n",
            "844/844 - 10s - 11ms/step - accuracy: 0.9892 - loss: 0.1414 - val_accuracy: 0.9933 - val_loss: 0.1261 - learning_rate: 1.0000e-03\n",
            "Epoch 13/20\n",
            "844/844 - 10s - 11ms/step - accuracy: 0.9902 - loss: 0.1282 - val_accuracy: 0.9903 - val_loss: 0.1445 - learning_rate: 1.0000e-03\n",
            "Epoch 14/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9912 - loss: 0.1246 - val_accuracy: 0.9940 - val_loss: 0.1260 - learning_rate: 1.0000e-03\n",
            "Epoch 15/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9916 - loss: 0.1182 - val_accuracy: 0.9922 - val_loss: 0.1188 - learning_rate: 1.0000e-03\n",
            "Epoch 16/20\n",
            "844/844 - 10s - 12ms/step - accuracy: 0.9920 - loss: 0.1162 - val_accuracy: 0.9823 - val_loss: 0.1986 - learning_rate: 1.0000e-03\n",
            "Epoch 17/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9916 - loss: 0.1164 - val_accuracy: 0.9878 - val_loss: 0.1350 - learning_rate: 1.0000e-03\n",
            "Epoch 18/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9921 - loss: 0.1144 - val_accuracy: 0.9915 - val_loss: 0.1211 - learning_rate: 1.0000e-03\n",
            "Epoch 19/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9955 - loss: 0.0759 - val_accuracy: 0.9927 - val_loss: 0.0706 - learning_rate: 5.0000e-04\n",
            "Epoch 20/20\n",
            "844/844 - 9s - 11ms/step - accuracy: 0.9956 - loss: 0.0614 - val_accuracy: 0.9940 - val_loss: 0.0707 - learning_rate: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n",
        "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Manually split into train and validation sets\n",
        "X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "history = model.fit(X_train_new, y_train_new,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=64,\n",
        "                    epochs=20,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "asvXPs1ABPXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02da997-eeb9-400a-f049-6bd492bc0234"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Digit Classification\n",
        "**Started with the Basics:**\n",
        "\n",
        "Understood how to load the MNIST dataset, normalize pixel values (0 to 1), and reshape images into the correct format for CNNs (28, 28, 1).\n",
        "\n",
        "**One-Hot Encoding:**\n",
        "\n",
        "Learned that label vectors must be converted to one-hot encoding for use with categorical_crossentropy loss in multi-class classification.\n",
        "\n",
        "**Simple Neural Networks (DNN):**\n",
        "\n",
        "Began with fully connected dense layers using ReLU and softmax. Achieved ~98% accuracy but noticed overfitting and poor generalization.\n",
        "\n",
        "**Introduced CNN Architecture:**\n",
        "\n",
        "Switched to convolutional neural networks which extract spatial features better.\n",
        "\n",
        "Used Conv2D, MaxPooling2D, and Flatten layers\n",
        "\n",
        "Accuracy improved significantly (~99.1%)\n",
        "\n",
        "**Added Regularization Techniques:**\n",
        "\n",
        "Dropout layers were added after convolution and dense layers to reduce overfitting.\n",
        "\n",
        "L2 Regularization was used in the dense layer to penalize large weights.\n",
        "\n",
        "**Used Batch Normalization:**\n",
        "\n",
        "Added after most layers to stabilize and accelerate training by normalizing activations.\n",
        "\n",
        "Helped the model converge faster and improved accuracy.\n",
        "\n",
        "**Applied Data Augmentation:**\n",
        "\n",
        "Used ImageDataGenerator to introduce variability (rotations, shifts, zooms) into training data.\n",
        "\n",
        "Improved robustness and generalization of the model (~99.4–99.5% accuracy).\n",
        "\n",
        "**Used Learning Rate Scheduling:**\n",
        "\n",
        "Added ReduceLROnPlateau callback to reduce learning rate when validation loss plateaued.\n",
        "\n",
        "Helped in fine-tuning the model in later epochs.\n",
        "\n",
        "**Implemented Early Stopping:**\n",
        "\n",
        "Prevented overfitting by stopping training when validation loss stopped improving.\n",
        "\n",
        "**Final Testing and Evaluation:**\n",
        "\n",
        "Evaluated the trained model on the test set using model.evaluate()\n",
        "\n",
        "Achieved high accuracy (~99.4%) without using ensembles or transfer learning."
      ],
      "metadata": {
        "id": "kRLBDpknZy3-"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}